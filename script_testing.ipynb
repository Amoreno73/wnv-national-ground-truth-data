{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6fc6e0",
   "metadata": {},
   "source": [
    "Context: parameters relevant to the propagation of the West Nile Virus.\n",
    "\n",
    "Years: 2017 to 2024\n",
    "\n",
    "Months: July to September\n",
    "\n",
    "Aggregation: yearly mean (July to September only for each year).\n",
    "\n",
    "Parameters: \n",
    "* Climate -> Relative Humidity (2m), Precipitation, Max Temperature (2m), Min Temperature (2m), Wind Speed (10m). \n",
    "* Water -> Land Surface Temperature Day, Land Surface Temperature Night, Chlorophyll\n",
    "* NDVI (mean) -> Agricultural (or croplands/farmlands), Forested land cover (Deciduous forest, Evergreen forest, Mixed forest), Rangeland (Grassland, Shrubland, Pasture, Natural grazing land) \n",
    "\n",
    "Region: County-level in the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f2ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d785c",
   "metadata": {},
   "source": [
    "### OpenMeteo API - Obtainable Parameters:\n",
    "* Climate -> Relative Humidity (2m), Precipitation, Max Temperature (2m), Min Temperature (2m), Wind Speed (10m). \n",
    "* Need source geoids (county fips codes) and the corresponding lat/lon using the TIGER API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38fd81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENSUS_COUNTIES = \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_500k.zip\"\n",
    "# connecticut used counties up until 2022, then switched to planning regions, which my WNV case data reflects\n",
    "# thus, I will get the lat/lon and corresponding data for both time periods (and FIPS code variations). \n",
    "LEGACY_COUNTIES_CT = \"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_county_500k.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60bf0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"https://raw.githubusercontent.com/Amoreno73/wnv_embeddings/refs/heads/main/notebooks/national_embeddings/national_wnv_case_data/all_data_normalized.csv\")\n",
    "df_all[\"GEOID\"] = df_all[\"GEOID\"].astype(str).str.zfill(5)\n",
    "counties = gpd.read_file(CENSUS_COUNTIES).to_crs(4326)\n",
    "legacy_counties = gpd.read_file(LEGACY_COUNTIES_CT).to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce2731ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_subset = legacy_counties[legacy_counties[\"GEOID\"].str.startswith(\"09\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49e56c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFP</th>\n",
       "      <th>COUNTYFP</th>\n",
       "      <th>COUNTYNS</th>\n",
       "      <th>GEOIDFQ</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>NAMELSAD</th>\n",
       "      <th>STUSPS</th>\n",
       "      <th>STATE_NAME</th>\n",
       "      <th>LSAD</th>\n",
       "      <th>ALAND</th>\n",
       "      <th>AWATER</th>\n",
       "      <th>geometry</th>\n",
       "      <th>AFFGEOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>003</td>\n",
       "      <td>00161527</td>\n",
       "      <td>0500000US01003</td>\n",
       "      <td>01003</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>06</td>\n",
       "      <td>4117725048</td>\n",
       "      <td>1132887203</td>\n",
       "      <td>POLYGON ((-88.02858 30.22676, -88.02399 30.230...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>069</td>\n",
       "      <td>00161560</td>\n",
       "      <td>0500000US01069</td>\n",
       "      <td>01069</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston County</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>06</td>\n",
       "      <td>1501742235</td>\n",
       "      <td>4795415</td>\n",
       "      <td>POLYGON ((-85.71209 31.19727, -85.70934 31.198...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>005</td>\n",
       "      <td>00161528</td>\n",
       "      <td>0500000US01005</td>\n",
       "      <td>01005</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>06</td>\n",
       "      <td>2292160151</td>\n",
       "      <td>50523213</td>\n",
       "      <td>POLYGON ((-85.74803 31.61918, -85.74544 31.618...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>119</td>\n",
       "      <td>00161585</td>\n",
       "      <td>0500000US01119</td>\n",
       "      <td>01119</td>\n",
       "      <td>Sumter</td>\n",
       "      <td>Sumter County</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>06</td>\n",
       "      <td>2340898915</td>\n",
       "      <td>24634880</td>\n",
       "      <td>POLYGON ((-88.41492 32.36452, -88.41471 32.366...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>091</td>\n",
       "      <td>00069166</td>\n",
       "      <td>0500000US05091</td>\n",
       "      <td>05091</td>\n",
       "      <td>Miller</td>\n",
       "      <td>Miller County</td>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>06</td>\n",
       "      <td>1616257232</td>\n",
       "      <td>36848741</td>\n",
       "      <td>POLYGON ((-94.04343 33.55158, -94.04332 33.552...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>09</td>\n",
       "      <td>011</td>\n",
       "      <td>00212799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09011</td>\n",
       "      <td>New London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06</td>\n",
       "      <td>1722716728</td>\n",
       "      <td>276657755</td>\n",
       "      <td>POLYGON ((-72.46673 41.5839, -72.42886 41.5889...</td>\n",
       "      <td>0500000US09011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>09</td>\n",
       "      <td>005</td>\n",
       "      <td>00212796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09005</td>\n",
       "      <td>Litchfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06</td>\n",
       "      <td>2384116952</td>\n",
       "      <td>62334525</td>\n",
       "      <td>POLYGON ((-73.51795 41.67086, -73.51678 41.687...</td>\n",
       "      <td>0500000US09005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>09</td>\n",
       "      <td>013</td>\n",
       "      <td>00212668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09013</td>\n",
       "      <td>Tolland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06</td>\n",
       "      <td>1062807467</td>\n",
       "      <td>17549693</td>\n",
       "      <td>POLYGON ((-72.51733 41.8699, -72.51692 41.8736...</td>\n",
       "      <td>0500000US09013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>09</td>\n",
       "      <td>015</td>\n",
       "      <td>00212801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09015</td>\n",
       "      <td>Windham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06</td>\n",
       "      <td>1328478475</td>\n",
       "      <td>21477921</td>\n",
       "      <td>POLYGON ((-72.25208 41.72706, -72.25264 41.728...</td>\n",
       "      <td>0500000US09015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>09</td>\n",
       "      <td>003</td>\n",
       "      <td>00212338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09003</td>\n",
       "      <td>Hartford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06</td>\n",
       "      <td>1903543897</td>\n",
       "      <td>40543777</td>\n",
       "      <td>POLYGON ((-73.02054 42.00009, -73.00876 42.038...</td>\n",
       "      <td>0500000US09003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3243 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     STATEFP COUNTYFP  COUNTYNS         GEOIDFQ  GEOID        NAME  \\\n",
       "0         01      003  00161527  0500000US01003  01003     Baldwin   \n",
       "1         01      069  00161560  0500000US01069  01069     Houston   \n",
       "2         01      005  00161528  0500000US01005  01005     Barbour   \n",
       "3         01      119  00161585  0500000US01119  01119      Sumter   \n",
       "4         05      091  00069166  0500000US05091  05091      Miller   \n",
       "...      ...      ...       ...             ...    ...         ...   \n",
       "3238      09      011  00212799             NaN  09011  New London   \n",
       "3239      09      005  00212796             NaN  09005  Litchfield   \n",
       "3240      09      013  00212668             NaN  09013     Tolland   \n",
       "3241      09      015  00212801             NaN  09015     Windham   \n",
       "3242      09      003  00212338             NaN  09003    Hartford   \n",
       "\n",
       "            NAMELSAD STUSPS STATE_NAME LSAD       ALAND      AWATER  \\\n",
       "0     Baldwin County     AL    Alabama   06  4117725048  1132887203   \n",
       "1     Houston County     AL    Alabama   06  1501742235     4795415   \n",
       "2     Barbour County     AL    Alabama   06  2292160151    50523213   \n",
       "3      Sumter County     AL    Alabama   06  2340898915    24634880   \n",
       "4      Miller County     AR   Arkansas   06  1616257232    36848741   \n",
       "...              ...    ...        ...  ...         ...         ...   \n",
       "3238             NaN    NaN        NaN   06  1722716728   276657755   \n",
       "3239             NaN    NaN        NaN   06  2384116952    62334525   \n",
       "3240             NaN    NaN        NaN   06  1062807467    17549693   \n",
       "3241             NaN    NaN        NaN   06  1328478475    21477921   \n",
       "3242             NaN    NaN        NaN   06  1903543897    40543777   \n",
       "\n",
       "                                               geometry        AFFGEOID  \n",
       "0     POLYGON ((-88.02858 30.22676, -88.02399 30.230...             NaN  \n",
       "1     POLYGON ((-85.71209 31.19727, -85.70934 31.198...             NaN  \n",
       "2     POLYGON ((-85.74803 31.61918, -85.74544 31.618...             NaN  \n",
       "3     POLYGON ((-88.41492 32.36452, -88.41471 32.366...             NaN  \n",
       "4     POLYGON ((-94.04343 33.55158, -94.04332 33.552...             NaN  \n",
       "...                                                 ...             ...  \n",
       "3238  POLYGON ((-72.46673 41.5839, -72.42886 41.5889...  0500000US09011  \n",
       "3239  POLYGON ((-73.51795 41.67086, -73.51678 41.687...  0500000US09005  \n",
       "3240  POLYGON ((-72.51733 41.8699, -72.51692 41.8736...  0500000US09013  \n",
       "3241  POLYGON ((-72.25208 41.72706, -72.25264 41.728...  0500000US09015  \n",
       "3242  POLYGON ((-73.02054 42.00009, -73.00876 42.038...  0500000US09003  \n",
       "\n",
       "[3243 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counties = pd.concat([counties, legacy_subset], ignore_index=True)\n",
    "all_counties # now includes both legacy and new fips codes for CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c53ad1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STATEFP', 'COUNTYFP', 'COUNTYNS', 'GEOIDFQ', 'GEOID', 'NAME',\n",
       "       'NAMELSAD', 'STUSPS', 'STATE_NAME', 'LSAD', 'ALAND', 'AWATER',\n",
       "       'geometry', 'AFFGEOID'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counties.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dac4ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure county geometries are in WGS84 first\n",
    "all_counties = all_counties.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Compute centroids in projected CRS, then convert back to WGS84 for API\n",
    "centroids = all_counties.to_crs(\"EPSG:5070\").geometry.centroid.to_crs(\"EPSG:4326\")\n",
    "\n",
    "all_counties[\"longitude\"] = centroids.x\n",
    "all_counties[\"latitude\"] = centroids.y\n",
    "\n",
    "# Hard guard before requests\n",
    "bad = all_counties[\n",
    "    ~all_counties[\"latitude\"].between(-90, 90) |\n",
    "    ~all_counties[\"longitude\"].between(-180, 180)\n",
    "]\n",
    "assert bad.empty, f\"Invalid lat/lon rows found: {len(bad)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0421d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['GEOID', 'NAME', 'STUSPS', 'STATE_NAME',\n",
    "       'geometry', 'latitude', 'longitude']\n",
    "\n",
    "all_counties = all_counties[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37312ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3150"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only the ones present in the national WNV embedding analysis. \n",
    "counties_interest = [c for c in set(all_counties[\"GEOID\"]) if c in set(df_all[\"GEOID\"])]\n",
    "len(counties_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5823cb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>STUSPS</th>\n",
       "      <th>STATE_NAME</th>\n",
       "      <th>geometry</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01003</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>POLYGON ((-88.02858 30.22676, -88.02399 30.230...</td>\n",
       "      <td>30.727083</td>\n",
       "      <td>-87.722293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01069</td>\n",
       "      <td>Houston</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>POLYGON ((-85.71209 31.19727, -85.70934 31.198...</td>\n",
       "      <td>31.153231</td>\n",
       "      <td>-85.302390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01005</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>POLYGON ((-85.74803 31.61918, -85.74544 31.618...</td>\n",
       "      <td>31.869536</td>\n",
       "      <td>-85.393439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01119</td>\n",
       "      <td>Sumter</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>POLYGON ((-88.41492 32.36452, -88.41471 32.366...</td>\n",
       "      <td>32.590808</td>\n",
       "      <td>-88.198762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05091</td>\n",
       "      <td>Miller</td>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>POLYGON ((-94.04343 33.55158, -94.04332 33.552...</td>\n",
       "      <td>33.312183</td>\n",
       "      <td>-93.892121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>09011</td>\n",
       "      <td>New London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-72.46673 41.5839, -72.42886 41.5889...</td>\n",
       "      <td>41.486562</td>\n",
       "      <td>-72.101497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>09005</td>\n",
       "      <td>Litchfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-73.51795 41.67086, -73.51678 41.687...</td>\n",
       "      <td>41.792223</td>\n",
       "      <td>-73.245412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>09013</td>\n",
       "      <td>Tolland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-72.51733 41.8699, -72.51692 41.8736...</td>\n",
       "      <td>41.854871</td>\n",
       "      <td>-72.336511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>09015</td>\n",
       "      <td>Windham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-72.25208 41.72706, -72.25264 41.728...</td>\n",
       "      <td>41.829889</td>\n",
       "      <td>-71.987442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>09003</td>\n",
       "      <td>Hartford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-73.02054 42.00009, -73.00876 42.038...</td>\n",
       "      <td>41.806230</td>\n",
       "      <td>-72.732865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3150 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GEOID        NAME STUSPS STATE_NAME  \\\n",
       "0     01003     Baldwin     AL    Alabama   \n",
       "1     01069     Houston     AL    Alabama   \n",
       "2     01005     Barbour     AL    Alabama   \n",
       "3     01119      Sumter     AL    Alabama   \n",
       "4     05091      Miller     AR   Arkansas   \n",
       "...     ...         ...    ...        ...   \n",
       "3238  09011  New London    NaN        NaN   \n",
       "3239  09005  Litchfield    NaN        NaN   \n",
       "3240  09013     Tolland    NaN        NaN   \n",
       "3241  09015     Windham    NaN        NaN   \n",
       "3242  09003    Hartford    NaN        NaN   \n",
       "\n",
       "                                               geometry   latitude  longitude  \n",
       "0     POLYGON ((-88.02858 30.22676, -88.02399 30.230...  30.727083 -87.722293  \n",
       "1     POLYGON ((-85.71209 31.19727, -85.70934 31.198...  31.153231 -85.302390  \n",
       "2     POLYGON ((-85.74803 31.61918, -85.74544 31.618...  31.869536 -85.393439  \n",
       "3     POLYGON ((-88.41492 32.36452, -88.41471 32.366...  32.590808 -88.198762  \n",
       "4     POLYGON ((-94.04343 33.55158, -94.04332 33.552...  33.312183 -93.892121  \n",
       "...                                                 ...        ...        ...  \n",
       "3238  POLYGON ((-72.46673 41.5839, -72.42886 41.5889...  41.486562 -72.101497  \n",
       "3239  POLYGON ((-73.51795 41.67086, -73.51678 41.687...  41.792223 -73.245412  \n",
       "3240  POLYGON ((-72.51733 41.8699, -72.51692 41.8736...  41.854871 -72.336511  \n",
       "3241  POLYGON ((-72.25208 41.72706, -72.25264 41.728...  41.829889 -71.987442  \n",
       "3242  POLYGON ((-73.02054 42.00009, -73.00876 42.038...  41.806230 -72.732865  \n",
       "\n",
       "[3150 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counties = all_counties[all_counties[\"GEOID\"].isin(counties_interest)]\n",
    "all_counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc004755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(all_counties[\"latitude\"].isna().sum())\n",
    "print(all_counties[\"longitude\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea49af7",
   "metadata": {},
   "source": [
    "I will be iterating over `all_counties` to get the mean yearly weather for each `GEOID`, using the Open-Meteo API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbaa3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_aggregate_by_year(data, year, county_id, lat, lon):\n",
    "    '''\n",
    "    Filter raw API data for a specific year (July-Sept) and aggregate to yearly metrics.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        Raw JSON response from Open-Meteo API\n",
    "    year : int\n",
    "        Year to filter for\n",
    "    county_id : str\n",
    "        GEOID of the county\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with one row containing aggregated yearly metrics\n",
    "    '''\n",
    "\n",
    "    # Define date range for this year's mosquito season\n",
    "    start_date = f\"{year}-07-01\"\n",
    "    end_date = f\"{year}-09-30\"\n",
    "\n",
    "    aggregated = {\n",
    "        'GEOID': county_id,\n",
    "        'year': year,\n",
    "        'latitude': lat,\n",
    "        'longitude': lon\n",
    "    }\n",
    "\n",
    "    # Process daily data (temperature)\n",
    "    if \"daily\" in data and data[\"daily\"]:\n",
    "        daily_df = pd.DataFrame(data[\"daily\"])\n",
    "        daily_df['time'] = pd.to_datetime(daily_df['time'])\n",
    "\n",
    "        # Filter for this year's July-Sept period\n",
    "        mask = (daily_df['time'] >= start_date) & (daily_df['time'] <= end_date)\n",
    "        daily_filtered = daily_df[mask]\n",
    "\n",
    "        if not daily_filtered.empty:\n",
    "            aggregated['mean_temp_2m_max'] = daily_filtered['temperature_2m_max'].mean()\n",
    "            aggregated['mean_temp_2m_min'] = daily_filtered['temperature_2m_min'].mean()\n",
    "        else:\n",
    "            aggregated['mean_temp_2m_max'] = None\n",
    "            aggregated['mean_temp_2m_min'] = None\n",
    "    else:\n",
    "        aggregated['mean_temp_2m_max'] = None\n",
    "        aggregated['mean_temp_2m_min'] = None\n",
    "\n",
    "    # Process hourly data (humidity, wind, precipitation)\n",
    "    if \"hourly\" in data and data[\"hourly\"]:\n",
    "        hourly_df = pd.DataFrame(data[\"hourly\"])\n",
    "        hourly_df['time'] = pd.to_datetime(hourly_df['time'])\n",
    "\n",
    "        # Filter for this year's July-Sept period\n",
    "        mask = (hourly_df['time'] >= start_date) & (hourly_df['time'] <= end_date)\n",
    "        hourly_filtered = hourly_df[mask].copy()\n",
    "\n",
    "        if not hourly_filtered.empty:\n",
    "            # Add date column for daily aggregation\n",
    "            hourly_filtered['date'] = hourly_filtered['time'].dt.date\n",
    "\n",
    "            # Aggregate hourly data to daily first\n",
    "            daily_agg = hourly_filtered.groupby('date').agg({\n",
    "                'relative_humidity_2m': 'mean',\n",
    "                'wind_speed_10m': 'mean',\n",
    "                'precipitation': 'sum'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Then take mean across all days in the period\n",
    "            aggregated['mean_humidity_2m'] = daily_agg['relative_humidity_2m'].mean()\n",
    "            aggregated['mean_wind_speed_10m'] = daily_agg['wind_speed_10m'].mean()\n",
    "            aggregated['mean_precipitation'] = daily_agg['precipitation'].mean()  # Mean daily precipitation\n",
    "        else:\n",
    "            aggregated['mean_humidity_2m'] = None\n",
    "            aggregated['mean_wind_speed_10m'] = None\n",
    "            aggregated['mean_precipitation'] = None\n",
    "    else:\n",
    "        aggregated['mean_humidity_2m'] = None\n",
    "        aggregated['mean_wind_speed_10m'] = None\n",
    "        aggregated['mean_precipitation'] = None\n",
    "\n",
    "    return pd.DataFrame([aggregated])\n",
    "\n",
    "\n",
    "def request_with_backoff(url, params, max_retries=8, base_wait_seconds=65):\n",
    "    '''\n",
    "    Robust request wrapper for Open-Meteo rate limiting.\n",
    "    Retries on HTTP 429 and API payload errors mentioning limit exceeded.\n",
    "    '''\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, params=params, timeout=60)\n",
    "\n",
    "        # HTTP-level rate limit\n",
    "        if response.status_code == 429:\n",
    "            wait = min(120, base_wait_seconds + attempt * 10) + np.random.uniform(0, 2)\n",
    "            print(f\"Rate limited (HTTP 429). Waiting {wait:.1f}s then retrying...\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # API-level error payload\n",
    "        try:\n",
    "            payload = response.json()\n",
    "            if isinstance(payload, dict) and payload.get(\"error\") and \"limit exceeded\" in str(payload.get(\"reason\", \"\")).lower():\n",
    "                wait = min(120, base_wait_seconds + attempt * 10) + np.random.uniform(0, 2)\n",
    "                print(f\"Rate limited ({payload.get('reason')}). Waiting {wait:.1f}s then retrying...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "        except Exception:\n",
    "            payload = None\n",
    "\n",
    "        response.raise_for_status()\n",
    "        return payload if payload is not None else response.json()\n",
    "\n",
    "    raise RuntimeError(f\"Rate limit retries exhausted after {max_retries} attempts\")\n",
    "\n",
    "\n",
    "def fetch_county_all_years(row, years=range(2017, 2025), save_dir=\"weather_data\"):\n",
    "    '''\n",
    "    Fetch ALL years for a single county in ONE API call.\n",
    "    Caches each year separately for resumability.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    row : Series\n",
    "        Must contain: GEOID, latitude, longitude\n",
    "    years : range\n",
    "        Years to fetch (default: 2017-2024)\n",
    "    save_dir : str\n",
    "        Directory to cache results\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with one row per year, aggregated metrics\n",
    "    '''\n",
    "\n",
    "    county_id = row[\"GEOID\"]\n",
    "    lat = row[\"latitude\"]\n",
    "    lon = row[\"longitude\"]\n",
    "\n",
    "    # Guard against bad CRS / projected coordinates\n",
    "    if not (-90 <= float(lat) <= 90) or not (-180 <= float(lon) <= 180):\n",
    "        print(f\"Skipping county {county_id}: invalid lat/lon ({lat}, {lon})\")\n",
    "        return None\n",
    "\n",
    "    # Check if all years are already cached\n",
    "    all_cached = True\n",
    "    cached_results = []\n",
    "\n",
    "    for year in years:\n",
    "        filename = os.path.join(save_dir, f\"county_{county_id}_year_{year}.parquet\")\n",
    "        if os.path.exists(filename):\n",
    "            try:\n",
    "                cached_results.append(pd.read_parquet(filename))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read cache for county {county_id}, year {year}: {e}\")\n",
    "                all_cached = False\n",
    "                break\n",
    "        else:\n",
    "            all_cached = False\n",
    "            break\n",
    "\n",
    "    # If all years cached, return immediately\n",
    "    if all_cached and cached_results:\n",
    "        return pd.concat(cached_results, ignore_index=True)\n",
    "\n",
    "    # Make API call for entire date range (all years at once)\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": f\"{min(years)}-07-01\",  # July 1 of first year\n",
    "        \"end_date\": f\"{max(years)}-09-30\",    # Sept 30 of last year\n",
    "        \"daily\": [\"temperature_2m_max\", \"temperature_2m_min\"],\n",
    "        \"hourly\": [\"relative_humidity_2m\", \"wind_speed_10m\", \"precipitation\"],\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # small per-request delay for gentler pacing\n",
    "        time.sleep(0.2)\n",
    "        data = request_with_backoff(url, params)\n",
    "\n",
    "        # Check if data is valid\n",
    "        if \"hourly\" not in data and \"daily\" not in data:\n",
    "            raise ValueError(f\"No weather data returned for county {county_id}\")\n",
    "\n",
    "        # Process and split by year\n",
    "        results = []\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        for year in years:\n",
    "            # Filter and aggregate for this specific year\n",
    "            year_aggregated = filter_and_aggregate_by_year(data, year, county_id, lat, lon)\n",
    "            results.append(year_aggregated)\n",
    "\n",
    "            # Cache each year separately (for resumability)\n",
    "            filename = os.path.join(save_dir, f\"county_{county_id}_year_{year}.parquet\")\n",
    "            year_aggregated.to_parquet(filename, index=False)\n",
    "\n",
    "        return pd.concat(results, ignore_index=True)\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout for county {county_id} at ({lat}, {lon})\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for county {county_id}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for county {county_id} at ({lat}, {lon}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_missing_counties(counties_df, years, save_dir=\"weather_data\"):\n",
    "    '''\n",
    "    Identify counties that need data (not fully cached).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    counties_df : DataFrame\n",
    "        Must have GEOID column\n",
    "    years : range\n",
    "        Years to check\n",
    "    save_dir : str\n",
    "        Cache directory\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame of counties that need fetching\n",
    "    '''\n",
    "\n",
    "    missing_counties = []\n",
    "\n",
    "    for _, row in counties_df.iterrows():\n",
    "        county_id = row['GEOID']\n",
    "\n",
    "        # Check if all years are cached\n",
    "        all_years_cached = True\n",
    "        for year in years:\n",
    "            filename = os.path.join(save_dir, f\"county_{county_id}_year_{year}.parquet\")\n",
    "            if not os.path.exists(filename):\n",
    "                all_years_cached = False\n",
    "                break\n",
    "\n",
    "        if not all_years_cached:\n",
    "            missing_counties.append(row)\n",
    "\n",
    "    if missing_counties:\n",
    "        return pd.DataFrame(missing_counties)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def chunk_dataframe(df, batch_size):\n",
    "    '''Yield DataFrame chunks of size batch_size.'''\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        yield df.iloc[start:start + batch_size]\n",
    "\n",
    "\n",
    "def fetch_all_weather_data(\n",
    "    counties_df,\n",
    "    years=range(2017, 2025),\n",
    "    max_workers=1,\n",
    "    save_dir=\"weather_data\",\n",
    "    batch_size=25,\n",
    "    batch_pause_seconds=65,\n",
    "):\n",
    "    '''\n",
    "    Fetch weather data for all counties across all years with batching + throttling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    counties_df : DataFrame\n",
    "        Must have columns: GEOID, latitude, longitude\n",
    "    years : range\n",
    "        Years to fetch (default: 2017-2024)\n",
    "    max_workers : int\n",
    "        Number of parallel API requests (recommended 1-2 to avoid throttling)\n",
    "    save_dir : str\n",
    "        Directory to cache results\n",
    "    batch_size : int\n",
    "        Number of counties processed per batch before pausing\n",
    "    batch_pause_seconds : int\n",
    "        Sleep time between batches\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with columns: GEOID, year, mean_temp_2m_max, mean_temp_2m_min,\n",
    "                           mean_humidity_2m, mean_wind_speed_10m, mean_precipitation\n",
    "    '''\n",
    "\n",
    "    print(f\"Total counties: {len(counties_df)}\")\n",
    "    print(f\"Years: {list(years)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Batch pause: {batch_pause_seconds}s\")\n",
    "\n",
    "    # Check what's already cached\n",
    "    missing_counties = get_missing_counties(counties_df, years, save_dir)\n",
    "\n",
    "    if missing_counties.empty:\n",
    "        print(\"All data already cached! Loading from cache...\")\n",
    "    else:\n",
    "        print(f\"Counties needing fetch: {len(missing_counties)}/{len(counties_df)}\")\n",
    "\n",
    "    failed_counties = []\n",
    "\n",
    "    if not missing_counties.empty:\n",
    "        batches = list(chunk_dataframe(missing_counties.reset_index(drop=True), batch_size))\n",
    "\n",
    "        for batch_idx, batch_df in enumerate(batches, start=1):\n",
    "            print(f\"\\nBatch {batch_idx}/{len(batches)} | counties: {len(batch_df)}\")\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(fetch_county_all_years, row, years, save_dir): row['GEOID']\n",
    "                    for _, row in batch_df.iterrows()\n",
    "                }\n",
    "\n",
    "                for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Batch {batch_idx}\"):\n",
    "                    county_id = futures[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        if result is None or result.empty:\n",
    "                            failed_counties.append(county_id)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Task failed for county {county_id}: {e}\")\n",
    "                        failed_counties.append(county_id)\n",
    "\n",
    "            if batch_idx < len(batches):\n",
    "                print(f\"Pausing {batch_pause_seconds}s to respect API limits...\")\n",
    "                time.sleep(batch_pause_seconds)\n",
    "\n",
    "    # Load ALL data from cache (including newly fetched)\n",
    "    print(\"\\nLoading all data from cache...\")\n",
    "    all_results = []\n",
    "\n",
    "    for _, row in tqdm(counties_df.iterrows(), total=len(counties_df), desc=\"Loading cached data\"):\n",
    "        county_id = row['GEOID']\n",
    "        county_data = []\n",
    "\n",
    "        for year in years:\n",
    "            filename = os.path.join(save_dir, f\"county_{county_id}_year_{year}.parquet\")\n",
    "            if os.path.exists(filename):\n",
    "                try:\n",
    "                    county_data.append(pd.read_parquet(filename))\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load cache for county {county_id}, year {year}: {e}\")\n",
    "\n",
    "        if county_data:\n",
    "            all_results.append(pd.concat(county_data, ignore_index=True))\n",
    "\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        final_df = final_df.sort_values(['GEOID', 'year']).reset_index(drop=True)\n",
    "\n",
    "        final_df = final_df[[\n",
    "            'GEOID',\n",
    "            'mean_temp_2m_max', 'mean_temp_2m_min',\n",
    "            'mean_humidity_2m', 'mean_wind_speed_10m',\n",
    "            'mean_precipitation',\n",
    "            'year'\n",
    "        ]]\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total rows: {len(final_df)}\")\n",
    "        print(f\"Unique counties: {final_df['GEOID'].nunique()}\")\n",
    "        print(f\"Years covered: {sorted(final_df['year'].unique())}\")\n",
    "        print(f\"Failed counties: {len(set(failed_counties))}\")\n",
    "\n",
    "        if failed_counties:\n",
    "            failed_unique = sorted(set(failed_counties))\n",
    "            print(f\"Failed county GEOIDs (sample): {failed_unique[:10]}{'...' if len(failed_unique) > 10 else ''}\")\n",
    "\n",
    "        missing_data = final_df.isnull().sum()\n",
    "        if missing_data.any():\n",
    "            print(\"\\nMissing values per column:\")\n",
    "            print(missing_data[missing_data > 0])\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    print(\"No data retrieved!\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def save_weather_summary(weather_df, output_path=\"county_weather_2017_2024.csv\"):\n",
    "    '''Save final weather data and print summary statistics.'''\n",
    "\n",
    "    weather_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nWeather data saved to: {output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / 1e6:.2f} MB\")\n",
    "\n",
    "    print(\"\\nSample data:\")\n",
    "    print(weather_df.head(10).to_string())\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Statistics by Year\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for year in sorted(weather_df['year'].unique()):\n",
    "        year_data = weather_df[weather_df['year'] == year]\n",
    "        print(f\"\\n{year}:\")\n",
    "        print(f\"  Counties: {len(year_data)}\")\n",
    "        print(f\"  Temp max: {year_data['mean_temp_2m_max'].mean():.1f} C (+/-{year_data['mean_temp_2m_max'].std():.1f})\")\n",
    "        print(f\"  Temp min: {year_data['mean_temp_2m_min'].mean():.1f} C (+/-{year_data['mean_temp_2m_min'].std():.1f})\")\n",
    "        print(f\"  Humidity: {year_data['mean_humidity_2m'].mean():.1f}% (+/-{year_data['mean_humidity_2m'].std():.1f})\")\n",
    "        print(f\"  Wind: {year_data['mean_wind_speed_10m'].mean():.1f} m/s (+/-{year_data['mean_wind_speed_10m'].std():.1f})\")\n",
    "        print(f\"  Precip: {year_data['mean_precipitation'].mean():.2f} mm/day (+/-{year_data['mean_precipitation'].std():.2f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b058ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3150 unique counties\n",
      "Total counties: 3150\n",
      "Years: [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Batch size: 40\n",
      "Batch pause: 20s\n",
      "Counties needing fetch: 3118/3150\n",
      "\n",
      "Batch 1/78 | counties: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  12%|█▎        | 5/40 [00:09<01:01,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited (HTTP 429). Waiting 65.2s then retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  18%|█▊        | 7/40 [00:13<00:59,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited (HTTP 429). Waiting 65.7s then retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  35%|███▌      | 14/40 [01:29<01:54,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited (HTTP 429). Waiting 65.2s then retrying...\n",
      "Rate limited (HTTP 429). Waiting 66.6s then retrying...\n",
      "Rate limited (HTTP 429). Waiting 76.0s then retrying...\n",
      "Rate limited (HTTP 429). Waiting 76.5s then retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  50%|█████     | 20/40 [04:04<03:04,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited (HTTP 429). Waiting 66.8s then retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  52%|█████▎    | 21/40 [04:05<02:11,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited (HTTP 429). Waiting 66.7s then retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  52%|█████▎    | 21/40 [05:14<04:44, 14.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited (HTTP 429). Waiting 65.1s then retrying...\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# USAGE EXAMPLE\n",
    "# ========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Load your counties data\n",
    "    # Assuming df_all has GEOID, latitude, longitude\n",
    "    # Get unique counties (avoid duplicates)\n",
    "    all_counties = all_counties[['GEOID', 'latitude', 'longitude']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Processing {len(all_counties)} unique counties\")\n",
    "    \n",
    "    # Fetch weather data\n",
    "    weather_df = fetch_all_weather_data(\n",
    "        counties_df=all_counties,\n",
    "        years=range(2017, 2025),  # 2017-2024 (8 years)\n",
    "        max_workers=1,  # keep low to avoid API throttling\n",
    "        save_dir=\"weather_data\",\n",
    "        batch_size=25,\n",
    "        batch_pause_seconds=65\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    if not weather_df.empty:\n",
    "        save_weather_summary(weather_df, output_path=\"county_weather_2017_2024.csv\")\n",
    "        \n",
    "        # Optional: Merge with main dataframe\n",
    "        print(\"\\nMerging with main dataframe...\")\n",
    "        \n",
    "        # Pivot to wide format (one row per GEOID)\n",
    "        weather_wide = weather_df.pivot(\n",
    "            index='GEOID',\n",
    "            columns='year',\n",
    "            values=['mean_temp_2m_max', 'mean_temp_2m_min', 'mean_humidity_2m', \n",
    "                    'mean_wind_speed_10m', 'mean_precipitation']\n",
    "        )\n",
    "        \n",
    "        # Flatten column names: mean_temp_2m_max_2017, mean_temp_2m_max_2018, ...\n",
    "        weather_wide.columns = [f'{col}_{year}' for col, year in weather_wide.columns]\n",
    "        weather_wide = weather_wide.reset_index()\n",
    "        \n",
    "        # Merge with main dataframe\n",
    "        df_all_with_weather = df_all.merge(weather_wide, on='GEOID', how='left')\n",
    "        \n",
    "        print(f\"✅ Final dataframe shape: {df_all_with_weather.shape}\")\n",
    "        print(f\"   Original columns: {df_all.shape[1]}\")\n",
    "        print(f\"   New weather columns: {weather_wide.shape[1] - 1}\")  # -1 for GEOID\n",
    "        \n",
    "        # Save merged dataframe\n",
    "        df_all_with_weather.to_csv(\"df_all_with_weather.csv\", index=False)\n",
    "        print(f\"✅ Merged dataframe saved to: df_all_with_weather.csv\")\n",
    "    else:\n",
    "        print(\"❌ No weather data retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_with_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d64ebd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
